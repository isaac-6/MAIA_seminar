{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing medical imaging segmentation like a pro (Part 1)\n",
    "*A highly oppiniated and biased tutorial on MRI lesion segmentation using Pytorch*\n",
    "\n",
    "**Sergi Valverde, PhD**\n",
    "*Universitat de Girona, Spain*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "---\n",
    "\n",
    "\n",
    "*Deep learning* techniques have been implemented for a wide range of computer vision and medical imaging tasks such as image registration, classification and segmentation, showing a superior performance in comparison with state-of-the-art available methods. In particular, *U-NET* like architectures are nowadays *the-facto* methods used in whatever medical imaging segmentation tasks required.  \n",
    "\n",
    "The goal of this tutorial is to introduce you to these new techniques. To do so, we will use MRI brain lesion segmentation as a context. Having the capability to hack and modify them for new problems will be a valuable contribution that you may want to leverage along of your research or professional career.\n",
    "\n",
    "The tutorial is divided in two parts: During the first part, I will introduce you to the awesome [Pytorch](http://pytorch.org) library. PyTorch is the most commonly used library for *deep learning* research. We will cover the basic concepts underlaying the library. As you will see, Altough Pytorch is a low-level library, it introduces a very *pythonic* and easy-to-use syntaxis, which permits to modify our models extensively and to introduce new ideas very fast. \n",
    "\n",
    "During the second part, we will implement the *U-NET* model and we will apply it to the MRI white matter lesion segmentation problem. We will cover the entire training and inference procedures, showing some tricks to learn better and faster models. Finally, I will introduce some of the latest techniques that have been proposed in the context of medical image segmentation, showing how easy is to incorporate them into our models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why PyTorch?\n",
    "\n",
    "---\n",
    "\n",
    "PyTorch is a strong player in the field of deep learning and artificial intelligence, and it can be considered primarily as a research-first library. Some reasons to use PyTorch: \n",
    "\n",
    "* Pytorch is Pythonic (covered)\n",
    "* Pytorch is a low-level library but easy to hack (covered)\n",
    "* Pytorch is easy to debug (covered)\n",
    "* Data parallelism is straighforward (not covered)\n",
    "* Dynamic computational graph support (not covered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch is pythonic:\n",
    "\n",
    "Pytorch syntaxes for operations are very similar to Python code. This makes the code very readable and hackable. Compare the following code to compute the product between two matrices in both Python (`numpy`) and PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]\n",
      " [2. 2. 2. 2.]]\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.ones((4,4))\n",
    "B = np.ones((4,4)) * 2\n",
    "C = A * B\n",
    "print(C)\n",
    "\n",
    "import torch\n",
    "A = torch.ones((4,4))\n",
    "B = torch.ones((4,4)) * 2\n",
    "C = A * B\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, `torch` tensors can be converted into `numpy` arrays very easily using the NumPy bridge. The `torch` and `numpy` arrays will share their underlying memory locations (if located in the CPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch tensor: tensor([1., 1., 1., 1., 1.])\n",
      "numpy array: [1. 1. 1. 1. 1.]\n",
      "torch tensor: tensor([2., 2., 2., 2., 2.])\n",
      "numpy array: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print('torch tensor:', a)\n",
    "print('numpy array:', b)\n",
    "\n",
    "# adding implictly 1 to the torch tensor\n",
    "a.add_(1)\n",
    "print('torch tensor:', a)\n",
    "print('numpy array:', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, `numpy` arrays can be also converted to `torch` tensors, maintaining the same underlaying memory locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy array: [2. 2. 2. 2. 2.]\n",
      "torch tensor: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print('numpy array:', a)\n",
    "print('torch tensor:', b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch is low-level but easy to hack:\n",
    "---\n",
    "\n",
    "In constrast to other libraries like [Keras](https://keras.io), in PyTorch most of our codebase has to be built from scratch. However, this may be more a feature than a drawback in most situations where we need more control on the task at hand. Given the pythonic syntaxis, it is very easy to move one and build complex models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, moving from any CPU or GPU device is straighforward in `torch`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]], device='cuda:0')\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((4,4))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y # sum is performed in the GPU!\n",
    "print(z)\n",
    "print(z.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd:\n",
    "\n",
    "The autograd package is the central element in all neural networks in PyTorch. The `autograd` package provides automatic differentiation for all the operations on tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different (dynamic computational graph support). \n",
    "\n",
    "All `torch.tensor` elements have an attribute called `.requires_grad`, which controls if all the operations in the `tensor` are tracked or not during computations. When finished, by calling `.backward()` all the gradients will be computed automatically. The gradient for each `torch.tensor` will be accumulated in the `.grad` attribute. \n",
    "\n",
    "`torch.tensor` and `Function` (`+`, `-`, `torch.mul`, ...)  are interconnected and build up an acyclic graph, that encodes a complete history of computation. Each `torch.tensor` has a `.grad_fn` attribute that references a `Function` that has created the `torch.tensor`. Let's see an example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True) # True by default\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y) # see how grad_fn contains the '+' function\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients:\n",
    "\n",
    "Here it's when backprop is applied. We can easily compute the gradient of `out` with `out.backward()`, then see gradients of $\\dfrac{d(out)}{dx}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the result makes sense. We can write `out`as $out=\\dfrac{1}{4}\\sum_i z_i, z_i = 3(x_i + 2)^2 $,  so given that $x_i = 1$ then $out = 27$,  therefore: $\\dfrac{d_{out}}{dx_i} = \\dfrac{3}{2}(x_i + 2)$ and $\\dfrac{d_{out}}{dx_i} |_{x_i = 1} = \\dfrac{9}{2} = 4.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first CNN network in Pytorch:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our first Convolutional Neural Network (CNN) from scratch. To be fair with the history, let's build the `LeNet` network proposed by LeCunn in 1995 for digit recognition: \n",
    "\n",
    "![LeNet](media/mnist.png)\n",
    "\n",
    "Let's define the network first. Any differentiable object (a loss function, a layer or the same network) in PyTorch has to be defined using the `nn.Module` class, and has to incorporate at least a `forward` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, here we have introduced some common neural network layers and activations, all included in the `torch.nn` and the `Functional` modules:\n",
    "\n",
    "* `nn.Conv2d` layer: 2D CNN layer\n",
    "* `nn.Linear` layer: Linear 1D layer \n",
    "* `F.max_pool2d` layer: 2D max pooling\n",
    "* `F.relu` activation: non-linear RELU activations\n",
    "\n",
    "Now, let's initialize the neural network we have created: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net with 81194 parameters\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "params = list(net.parameters())\n",
    "num_params = sum([np.prod(p.size()) for p in params])\n",
    "print('Neural net with', num_params, 'parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Let's try a random 32x32 input and target to emulate a mini-batch training. We estimate how far away we are from the target using a loss function from the `nn.package`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2209, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# input and target\n",
    "input = torch.randn(1, 1, 32,32)\n",
    "target = torch.randn(10) # dummy probs for each label\n",
    "\n",
    "# network forward pass\n",
    "output = net(input)\n",
    "\n",
    "# Minimum square error criterion loss\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target.view(1, -1))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop:\n",
    "\n",
    "Finally, the last thing to do is to propagate the error with `loss.backward()`. Take into account that we need to clear the existing gradients for each minibatch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before backward: None\n",
      "after backward: tensor([ 0.0036, -0.0092, -0.0036,  0.0004, -0.0027, -0.0021])\n"
     ]
    }
   ],
   "source": [
    "# clear gradients\n",
    "net.zero_grad()\n",
    "\n",
    "# print the conv1 for instance\n",
    "print('before backward:', net.conv1.bias.grad)\n",
    "\n",
    "# forward pass\n",
    "output = net(input)\n",
    "\n",
    "# compute the loss and backprop the error\n",
    "loss = criterion(output, target.view(1, -1))\n",
    "loss.backward()\n",
    "print('after backward:', net.conv1.bias.grad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights of the model:\n",
    "\n",
    "So far, we are not updating the weights of the model. As you may know, the most used technique is Stochastic Gradient Descend (`SGD`). The update rule is defined as:\n",
    "\n",
    "$$ weight = weight - learning_{rate} * gradient$$\n",
    "\n",
    "So we could manually update all the model weights as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, PyTorch incorporates the package `torch.optim` that implements most of the state-the-art optimizers available today. Using `torch.optim`, we can finally define the entire training loop for a particular mini-batch of data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# clear gradients\n",
    "net.zero_grad()\n",
    "\n",
    "# forward pass\n",
    "output = net(input)\n",
    "\n",
    "# compute the loss and backprop the error\n",
    "loss = criterion(output, target.view(1, -1))\n",
    "loss.backward()\n",
    "\n",
    "# update the weights of the network\n",
    "optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
